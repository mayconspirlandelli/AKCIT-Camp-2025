# ============================================================
# TechAdvisor ‚Äì Agente conversacional com LangChain + LangGraph
# ============================================================
# Objetivo did√°tico:
# - Mostrar como conectar um LLM (Gemini) usando a integra√ß√£o moderna `langchain-google-genai`.
# - Ensinar a criar um `PromptTemplate` e compor uma pipeline com LCEL: `prompt | llm | parser`.
# - Demonstrar a orquestra√ß√£o de um fluxo com m√∫ltiplos n√≥s no LangGraph (`StateGraph`).
# - Rodar de forma interativa no terminal, guiando o usu√°rio por boas‚Äëvindas, coleta de nome e Q&A.

import os
import re
from dotenv import load_dotenv
from langchain.prompts import PromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.output_parsers import StrOutputParser
from langgraph.graph import StateGraph, END

# ============================================================
# 1. Carregar vari√°veis de ambiente
# ============================================================
# Busca um arquivo `.env` na raiz do projeto e carrega as vari√°veis
# (por exemplo, GOOGLE_API_KEY). Assim, n√£o precisamos exportar
# manualmente no terminal a cada execu√ß√£o.
load_dotenv()

# Garante que a chave esteja dispon√≠vel via ambiente
os.environ.get("GOOGLE_API_KEY")

# ============================================================
# 2. Definir o LLM (Gemini via langchain-google-genai)
# ============================================================
# `ChatGoogleGenerativeAI` √© o wrapper do LangChain para modelos Gemini.
# Par√¢metros principais:
# - model: nome do modelo (ajuste para o que sua conta permite).
# - temperature: controla a criatividade (0 = mais determin√≠stico; 1 = mais criativo).
llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash",
    temperature=0.7
)

# ============================================================
# 3. Definir o PromptTemplate (para Q&A ap√≥s conhecer o nome)
# ============================================================
# O PromptTemplate permite parametrizar trechos do prompt, como `{interesse}`.
# No momento de executar, substituiremos esse placeholder pelo valor fornecido
# pelo usu√°rio no estado do LangGraph.
template_text = (
    "Voc√™ √© o TechAdvisor, um especialista amig√°vel em tecnologia e programa√ß√£o.\n"
    "Converse de forma objetiva, em portugu√™s, com o usu√°rio {nome}.\n"
    "Pergunta do usu√°rio: {pergunta}\n\n"
    "Responda de forma curta e √∫til. Quando adequado, recomende tecnologias, frameworks, "
    "boas pr√°ticas ou pr√≥ximos passos de estudo."
)
prompt = PromptTemplate(
    input_variables=["nome", "pergunta"],
    template=template_text
)

# ============================================================
# 4. Criar uma pipeline LCEL (Prompt -> Modelo -> Parser) para Q&A
# ============================================================
# LCEL (LangChain Expression Language) permite compor etapas como um pipeline.
# Aqui encadeamos:
#   1) prompt: recebe `{interesse}` e gera a string final de instru√ß√£o
#   2) llm: chama o modelo de chat do Gemini com esse prompt
#   3) StrOutputParser(): converte a resposta para uma string simples
qa_chain = prompt | llm | StrOutputParser()

# ============================================================
# 5. Integrar a pipeline dentro de um fluxo com LangGraph (m√∫ltiplos n√≥s)
# ============================================================
# O estado √© um dicion√°rio com chaves como:
# - etapa: controla o pr√≥ximo n√≥ ("boas_vindas" | "aguardar_nome" | "responder_perguntas" | "fim")
# - mensagem_usuario: √∫ltima entrada do usu√°rio
# - nome: nome do usu√°rio, quando capturado
# - resposta: √∫ltima resposta do agente (sa√≠da para UI/CLI)
# - historico: lista opcional de trocas (para futura expans√£o)

def extrair_nome(texto: str) -> str:
    """Heur√≠stica simples: usa a frase inteira como nome, limpando espa√ßos e pontua√ß√£o leve."""
    if not texto:
        return ""
    candidato = texto.strip()
    # Remove aspas e pontua√ß√£o nas pontas
    candidato = re.sub(r'^["\'\s]+|["\'\s]+$', "", candidato)
    # Capitaliza primeiro nome/partes
    partes = [p.capitalize() for p in re.split(r"\s+", candidato) if p]
    return " ".join(partes)


def boas_vindas_node(state: dict) -> dict:
    state["resposta"] = "Ol√°! Eu sou o TechAdvisor. Como posso te chamar?"
    state["etapa"] = "aguardar_nome"
    return state


def aguardar_nome_node(state: dict) -> dict:
    mensagem = (state.get("mensagem_usuario") or "").strip()
    if not mensagem:
        state["resposta"] = "N√£o entendi. Qual √© o seu nome?"
        state["etapa"] = "aguardar_nome"
        return state

    nome = extrair_nome(mensagem)
    if not nome:
        state["resposta"] = "Poderia repetir seu nome, por favor?"
        state["etapa"] = "aguardar_nome"
        return state

    state["nome"] = nome
    state.setdefault("historico", [])
    state["resposta"] = f"Prazer, {nome}! Como posso ajudar em tecnologia hoje?"
    state["etapa"] = "responder_perguntas"
    return state


def responder_perguntas_node(state: dict) -> dict:
    mensagem = (state.get("mensagem_usuario") or "").strip()
    nome = state.get("nome", "usu√°rio")

    if mensagem.lower() == "tchau" or "tchau" in mensagem.lower():
        state["resposta"] = f"At√© logo, {nome}! üëã"
        state["etapa"] = "fim"
        state["encerrar"] = True
        return state

    # Gera resposta via LLM
    resposta = qa_chain.invoke({
        "nome": nome,
        "pergunta": mensagem or "Me diga algo legal sobre tecnologia."
    })

    # Atualiza hist√≥rico simples
    historico = state.setdefault("historico", [])
    if mensagem:
        historico.append({"role": "user", "content": mensagem})
    historico.append({"role": "assistant", "content": resposta})

    state["resposta"] = resposta
    state["etapa"] = "responder_perguntas"
    state["encerrar"] = False
    return state


def roteador_node(state: dict) -> dict:
    # N√≥ "in√≥cuo" apenas para permitir arestas condicionais
    return state


def proxima_parada(state: dict) -> str:
    # Verifica a etapa atual do estado
    etapa = state.get("etapa")
    if etapa == "aguardar_nome":
        return "aguardar_nome"
    if etapa == "responder_perguntas":
        return "responder_perguntas"
    if etapa == "fim":
        return "fim"
    # Padr√£o: in√≠cio do fluxo
    return "boas_vindas"


# Criar o grafo do agente com m√∫ltiplos n√≥s
graph = StateGraph(dict)

graph.add_node("roteador", roteador_node)
graph.add_node("boas_vindas", boas_vindas_node)
graph.add_node("aguardar_nome", aguardar_nome_node)
graph.add_node("responder_perguntas", responder_perguntas_node)

# Configura o roteamento din√¢mico entre os n√≥s do grafo
# - O n√≥ "roteador" consulta a fun√ß√£o proxima_parada() para decidir o pr√≥ximo destino
# - A fun√ß√£o proxima_parada() verifica state["etapa"] e retorna uma das strings:
#   - "boas_vindas": inicia o fluxo com sauda√ß√£o
#   - "aguardar_nome": espera input do nome do usu√°rio 
#   - "responder_perguntas": processa perguntas via LLM
#   - "fim": encerra o fluxo (END)
# - As arestas condicionais mapeiam cada retorno ao n√≥ correspondente
graph.add_conditional_edges(
    "roteador",
    proxima_parada,
    {
        "boas_vindas": "boas_vindas",
        "aguardar_nome": "aguardar_nome",
        "responder_perguntas": "responder_perguntas",
        "fim": END,
    },
)

# Cada n√≥ executa uma "√∫nica rodada" e retorna para o chamador
graph.add_edge("boas_vindas", END)
graph.add_edge("aguardar_nome", END)
graph.add_edge("responder_perguntas", END)

graph.set_entry_point("roteador")

# Compila o grafo em um executor (cria um app pronto para .invoke)
app = graph.compile()

# ============================================================
# 6. Execu√ß√£o interativa (simula√ß√£o de uso)
# ============================================================
# Loop de CLI simples para interagir com o agente.
# A cada entrada do usu√°rio, invocamos o grafo passando um estado inicial
# com a chave "interesse" e depois exibimos a chave "resposta".
if __name__ == "__main__":
    print("ü§ñ TechAdvisor - Agente conversacional sobre tecnologia\n")

    # Estado de conversa persistente entre rodadas
    state: dict = {}

    # Primeira sauda√ß√£o
    result = app.invoke(state)
    state.update(result)
    print(state.get("resposta", "Ol√°!"))

    while True:
        try:
            mensagem = input("\nVoc√™: ")
        except (KeyboardInterrupt, EOFError):
            print("\nEncerrando o agente. At√© logo!")
            break

        if mensagem.lower() in ["sair", "exit", "quit"]:
            print("Encerrando o agente. At√© logo!")
            break

        # Preenche a √∫ltima mensagem e invoca 1 passo do grafo
        state["mensagem_usuario"] = mensagem
        result = app.invoke(state)
        state.update(result)

        print(f"\nüîé Agente: {state.get('resposta', '')}")

        if state.get("etapa") == "fim" or state.get("encerrar"):
            print("\nConversa encerrada.")
            break
